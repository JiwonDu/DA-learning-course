{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801b4c9c-6518-4909-bff6-80a950ddd103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 15:37:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"241211_02_SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f813175d-d223-4619-8e79-980fc6fe74e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trips_df = spark.read.csv('data/fhvhv_tripdata_2020-03.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7350a7b8-408a-413b-978d-2310e05a8584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2dbfa8-b0c8-4957-a365-12221698afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView('mobility_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6095461f-3fb1-41c0-bf03-0db12b8c8b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select *\n",
    "from mobility_data\n",
    "limit 5\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21fca5ac-7889-4d92-8cb1-ab964a0587af",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = '''\n",
    "select split(pickup_datetime, ' ')[0] as pickup_date, count(*) as trips\n",
    "from mobility_data\n",
    "group by pickup_date\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c72b09-6585-48f9-8ba9-9c7acef3424f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-16|391518|\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-06|872012|\n",
      "| 2020-03-26|141607|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-02|648986|\n",
      "| 2020-03-25|141088|\n",
      "| 2020-03-20|261900|\n",
      "| 2020-03-24|141686|\n",
      "| 2020-03-04|707879|\n",
      "| 2020-03-10|626474|\n",
      "| 2020-03-12|643257|\n",
      "| 2020-03-11|628601|\n",
      "| 2020-03-13|660914|\n",
      "| 2020-03-27|159339|\n",
      "| 2020-03-22|162165|\n",
      "| 2020-03-28|138456|\n",
      "| 2020-03-01|784246|\n",
      "| 2020-03-19|252773|\n",
      "| 2020-03-09|628940|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1671d5-e65b-4a1b-880a-f435269f95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = '''\n",
    "SELECT \n",
    "    pickup_date, \n",
    "    count(*) as trips\n",
    "FROM ( SELECT\n",
    "  split(pickup_datetime, ' ')[0] as pickup_date\n",
    "  FROM mobility_data )\n",
    "group by pickup_date\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36bab5a-f815-4e81-8115-677116598565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-16|391518|\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-06|872012|\n",
      "| 2020-03-26|141607|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-02|648986|\n",
      "| 2020-03-25|141088|\n",
      "| 2020-03-20|261900|\n",
      "| 2020-03-24|141686|\n",
      "| 2020-03-04|707879|\n",
      "| 2020-03-10|626474|\n",
      "| 2020-03-12|643257|\n",
      "| 2020-03-11|628601|\n",
      "| 2020-03-13|660914|\n",
      "| 2020-03-27|159339|\n",
      "| 2020-03-22|162165|\n",
      "| 2020-03-28|138456|\n",
      "| 2020-03-01|784246|\n",
      "| 2020-03-19|252773|\n",
      "| 2020-03-09|628940|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439b4504-5aa7-4d63-a003-9b149bfb7f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['split('pickup_datetime,  )[0] AS pickup_date#144, 'count(1) AS trips#145]\n",
      "+- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [split(pickup_datetime#18,  , -1)[0]], [split(pickup_datetime#18,  , -1)[0] AS pickup_date#144, count(1) AS trips#145L]\n",
      "+- SubqueryAlias mobility_data\n",
      "   +- Relation[hvfhs_license_num#16,dispatching_base_num#17,pickup_datetime#18,dropoff_datetime#19,PULocationID#20,DOLocationID#21,SR_Flag#22] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [split(pickup_datetime#18,  , -1)[0]], [split(pickup_datetime#18,  , -1)[0] AS pickup_date#144, count(1) AS trips#145L]\n",
      "+- Project [pickup_datetime#18]\n",
      "   +- Relation[hvfhs_license_num#16,dispatching_base_num#17,pickup_datetime#18,dropoff_datetime#19,PULocationID#20,DOLocationID#21,SR_Flag#22] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[split(pickup_datetime#18,  , -1)[0]#149], functions=[count(1)], output=[pickup_date#144, trips#145L])\n",
      "+- Exchange hashpartitioning(split(pickup_datetime#18,  , -1)[0]#149, 200), ENSURE_REQUIREMENTS, [id=#126]\n",
      "   +- *(1) HashAggregate(keys=[split(pickup_datetime#18,  , -1)[0] AS split(pickup_datetime#18,  , -1)[0]#149], functions=[partial_count(1)], output=[split(pickup_datetime#18,  , -1)[0]#149, count#151L])\n",
      "      +- FileScan csv [pickup_datetime#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/lab06/src/DA-learning-course/Spark/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_1).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdeed23-e8e6-40d1-b8d2-e1b14d339912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['pickup_date, 'count(1) AS trips#153]\n",
      "+- 'SubqueryAlias __auto_generated_subquery_name\n",
      "   +- 'Project ['split('pickup_datetime,  )[0] AS pickup_date#152]\n",
      "      +- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [pickup_date#152], [pickup_date#152, count(1) AS trips#153L]\n",
      "+- SubqueryAlias __auto_generated_subquery_name\n",
      "   +- Project [split(pickup_datetime#18,  , -1)[0] AS pickup_date#152]\n",
      "      +- SubqueryAlias mobility_data\n",
      "         +- Relation[hvfhs_license_num#16,dispatching_base_num#17,pickup_datetime#18,dropoff_datetime#19,PULocationID#20,DOLocationID#21,SR_Flag#22] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [pickup_date#152], [pickup_date#152, count(1) AS trips#153L]\n",
      "+- Project [split(pickup_datetime#18,  , -1)[0] AS pickup_date#152]\n",
      "   +- Relation[hvfhs_license_num#16,dispatching_base_num#17,pickup_datetime#18,dropoff_datetime#19,PULocationID#20,DOLocationID#21,SR_Flag#22] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[pickup_date#152], functions=[count(1)], output=[pickup_date#152, trips#153L])\n",
      "+- Exchange hashpartitioning(pickup_date#152, 200), ENSURE_REQUIREMENTS, [id=#148]\n",
      "   +- *(1) HashAggregate(keys=[pickup_date#152], functions=[partial_count(1)], output=[pickup_date#152, count#158L])\n",
      "      +- *(1) Project [split(pickup_datetime#18,  , -1)[0] AS pickup_date#152]\n",
      "         +- FileScan csv [pickup_datetime#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/lab06/src/DA-learning-course/Spark/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query_2).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8c172-d90c-4b7f-a295-3dacb0561a28",
   "metadata": {},
   "source": [
    "> 결론\n",
    "\n",
    "spark 환경에서는 쿼리에 따른 성능 차이가 크지 않음.  \n",
    "선후 정도만 고려하면, 나머지는 최적화된 형식으로 실행됨.  \n",
    "따라서, 쿼리의 최적화를 위해 무엇을 해야할까 고민할 때는 추천되는 플랜을 참고하여 바꿀 것.  \n",
    "\n",
    "즉,\n",
    "spark.sql의 카탈리스트 최적화 엔진에 의해 물리적으로 최적의 실행 계획 > 바이트 코드로 변환해서 실행\n",
    "\n",
    "BUT. 쿼리를 잘짜는 것은 중요. (왜냐하면, 늘 최적의 쿼리를 추천해주는 것은 아니기 때문)\n",
    "\n",
    "툴에 관심이 있다면, 개선 방안을 고민해보는 것을 추천."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dca9b-5819-475b-a532-79cad8ea1bcf",
   "metadata": {},
   "source": [
    "# ➕ 추가\n",
    "\n",
    "### 단일 쿼리 (Single Query)\n",
    "단일 쿼리는 한 번의 명령으로 데이터를 선택, 삽입, 업데이트 또는 삭제하는 SQL 명령입니다.\n",
    "\n",
    "**예시**:\n",
    "```sql\n",
    "SELECT title, author_lname\n",
    "FROM books\n",
    "WHERE stock_quantity < 30;\n",
    "이 쿼리는 books 테이블에서 재고가 30개 미만인 책의 제목과 작가의 성을 선택합니다.\n",
    "```\n",
    "### 서브쿼리 (Subquery)\n",
    "서브쿼리는 하나의 SQL 쿼리 내에 포함된 또 다른 쿼리입니다.  \n",
    "보통 메인 쿼리의 조건을 만족시키기 위해 사용됩니다.\n",
    "\n",
    "**예시**:\n",
    "\n",
    "```sql\n",
    "SELECT title, author_lname\n",
    "FROM books\n",
    "WHERE borrowed_by IN (\n",
    "    SELECT user_id\n",
    "    FROM users\n",
    "    WHERE address = '서울'\n",
    ");\n",
    "이 쿼리는 books 테이블에서 서울에 사는 사용자가 대여한 책의 제목과 작가의 성을 선택합니다.\n",
    "```\n",
    "### 다중 행 쿼리 (Multiple Rows Query)\n",
    "다중 행 쿼리는 여러 행을 반환하거나 업데이트할 때 사용됩니다.  \n",
    "일반적으로 집계 함수와 그룹화를 사용하여 여러 행을 처리합니다.\n",
    "\n",
    "**예시**:\n",
    "```sql\n",
    "SELECT author_lname, COUNT(*) AS num_books\n",
    "FROM books\n",
    "GROUP BY author_lname\n",
    "HAVING COUNT(*) > 1;\n",
    "이 쿼리는 books 테이블에서 각 작가의 성별로 그룹화하고, 두 권 이상의 책을 작성한 작가의 성과 책의 개수를 반환합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5cd7a2-ef4b-465e-9022-3b154e92b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e32a2-fa2c-47b2-b0c0-9a225cfd833c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
