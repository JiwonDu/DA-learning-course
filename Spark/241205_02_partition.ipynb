{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264bf259-f620-4bb6-bfc7-fedb820ae225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 10:37:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/05 10:37:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/05 10:37:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "# 스파크 환경 설정 객체 생성\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"241205_02_partition\")\n",
    "spark = SparkContext(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96115d66-b7d6-42bd-bb7c-185066281ae8",
   "metadata": {},
   "source": [
    "# 사용자가 지정하는 파티션\n",
    "\n",
    "```\n",
    "partitionBy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03c2245-acae-4942-b8ab-da81188ecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = spark.parallelize(\n",
    "    [\n",
    "        1,2,3,4,2,4,1\n",
    "    ]\n",
    ").map(lambda x: (x,x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea300d80-3dc3-4e4d-808f-c35d375bb3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 2), (3, 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be708b61-9a00-4a79-8126-07d3eeb10538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파티션 현황 확인 glom()\n",
    "\n",
    "pairs.partitionBy(2).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a0e798-f278-4dce-ae2f-c0c7ed06a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1), (2, 2), (3, 3), (4, 4), (2, 2), (4, 4), (1, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e199ed1-8e37-47bc-bbf1-79d1878ac59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파티션별로 규칙에 따라 데이터를 넣을 수 있다. > lambda 식으로 정의\n",
    "#파티션 배치함수\n",
    "\n",
    "pairs.partitionBy(2, lambda x: x%2).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a928e35a-3555-4ad9-9f79-ee3dee4dc6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pairs = pairs.partitionBy(2, lambda x: x%2).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d39515c-671a-4a88-9334-2e2d649bd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c90ef",
   "metadata": {},
   "source": [
    "## parallelize와 partitionBy의 차이\n",
    "> parallelize:\n",
    "\n",
    "- 설명: parallelize 함수는 주어진 리스트를 RDD로 변환하고, 이를 특정 개수의 파티션으로 나눕니다.\n",
    "\n",
    "- 예시: sample_rdd = spark.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 3)는 리스트 [1, 2, 3, 4, 5, 6, 7, 8, 9]를 RDD로 변환하고 3개의 파티션으로 나눕니다.\n",
    "\n",
    "- 결과:\n",
    "\n",
    "파티션 0: [1, 2, 3]\n",
    "\n",
    "파티션 1: [4, 5, 6]\n",
    "\n",
    "파티션 2: [7, 8, 9]\n",
    "\n",
    "> partitionBy:\n",
    "\n",
    "- 설명: partitionBy 함수는 키-값 쌍으로 이루어진 RDD를 특정 조건에 따라 파티셔닝합니다. 파티션 수와 파티션 함수를 사용하여 데이터를 나눕니다.\n",
    "\n",
    "- 예시: 키-값 쌍 RDD에서 pairs.partitionBy(2, lambda x: x % 2)는 각 키에 대해 2개의 파티션으로 나누는데, lambda x: x % 2 조건에 따라 파티셔닝합니다.\n",
    "\n",
    "- 결과:\n",
    "\n",
    "첫 번째 파티션: 짝수 키\n",
    "\n",
    "두 번째 파티션: 홀수 키\n",
    "\n",
    "> 요약\n",
    "\n",
    "- parallelize: 기본적으로 파티션 수만 지정하여 RDD를 생성하고 데이터를 분산시킵니다. 주로 데이터를 초기화하고 병렬 처리를 설정하는 데 사용됩니다.\n",
    "\n",
    "- partitionBy: 키-값 쌍 RDD에서 특정 조건에 따라 데이터를 나누는 데 사용됩니다. 주로 데이터 분산을 최적화하고 특정 조건에 따라 파티션을 나누는 데 사용됩니다.\n",
    "\n",
    "- 따라서 parallelize를 사용하여 RDD를 생성하고 파티션 수를 지정하면, 기본적인 병렬 처리 및 파티셔닝이 가능하므로 partitionBy를 추가로 사용할 필요는 없습니다. 하지만 키-값 쌍 RDD에서 특정 조건에 따라 파티셔닝을 해야 하는 경우 partitionBy를 사용하는 것이 유용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c914e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
