{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d678707-2927-4429-956b-e65363df18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad890ab3-0ff3-4bad-88d4-053a0f7e2369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 15:05:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#singleton pattern object builder\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"FirstSparkSessionApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f26a93-8735-4450-a537-652c333f7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c267a9b-70da-4990-9037-9b49fa17d733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6dd9a0-5b3a-4741-b966-1fa454d4beaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myRange.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e66ae-4592-40fd-ad41-4b7e53a17371",
   "metadata": {},
   "source": [
    "교재 46, 52페이지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d15a994-a684-4c85-9965-fe2a5f819ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Brook\", 20),\n",
    "    (\"Denny\", 31),\n",
    "    (\"Jules\", 30),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70427535-16d6-4cd6-b941-049330bc0d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = spark.createDataFrame(data)\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4fcbcca-3942-4b18-8e77-8591f2d68051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Brook| 20|\n",
      "|Denny| 31|\n",
      "|Jules| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be283529-584f-403b-ba14-efb20cbaab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fafee72-0456-4a73-862d-a1f0a266d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.createOrReplaceTempView(\"people\") #임시뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e8f11e-946d-43e5-9bf4-ddafcb8c6ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Denny| 31|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql 문 작성\n",
    "result = spark.sql(\"select _1, _2 from people where _2 > 30\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232a99b-de59-4358-90cd-c22f5314f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StructType으로 구조 정의 52페이지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "493ce07a-0022-4e08-abd1-e5feecc74048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a038bf7-56d9-412a-b028-301d7fc5ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"Author\", StringType(), False),\n",
    "        StructField(\"age\", IntegerType(), False)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feae39d9-b22b-4090-885f-0fffdfec4521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Author,StringType,false),StructField(age,IntegerType,false)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39265a62-2ad0-4d0f-ba21-8146a7cb0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fdc3046-23e3-4ad4-801c-2b5523b23646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Author: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e93ac2f3-fd55-479e-93ae-1fecb10babe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|Author|age|\n",
      "+------+---+\n",
      "| Brook| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df4ccd-4e07-465e-91a6-41f19d32f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDL 로 구조 정의 52페이지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51d7fa09-2f9c-43ac-8a54-4d04d41b3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = \"\"\"\n",
    "    ID INT, \n",
    "    First STRING, \n",
    "    Last STRING, \n",
    "    Url STRING, \n",
    "    Published STRING, \n",
    "    Hits INT, \n",
    "    Campaigns ARRAY<STRING>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b545162d-6b82-43f6-8947-6b96e780ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema for our data\n",
    "schema = StructType([\n",
    "   StructField(\"Id\", IntegerType(), False),\n",
    "   StructField(\"First\", StringType(), False),\n",
    "   StructField(\"Last\", StringType(), False),\n",
    "   StructField(\"Url\", StringType(), False),\n",
    "   StructField(\"Published\", StringType(), False),\n",
    "   StructField(\"Hits\", IntegerType(), False),\n",
    "   StructField(\"Campaigns\", ArrayType(StringType()), False)])\n",
    "\n",
    "#create our data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "       [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "       [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "       [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a3afd3-a567-4958-9ff3-4a8177f16e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d267f000-4a4f-4df8-87d3-e00e6c6b5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df1 = spark.createDataFrame(data, schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70f88ccd-a5bb-42d3-b501-db0926d85146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94455da0-7b84-47da-8c3d-03455c390072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = false)\n",
      " |-- First: string (nullable = false)\n",
      " |-- Last: string (nullable = false)\n",
      " |-- Url: string (nullable = false)\n",
      " |-- Published: string (nullable = false)\n",
      " |-- Hits: integer (nullable = false)\n",
      " |-- Campaigns: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a46d9cf6-4b0d-4a70-a720-f6843d900590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#column 별로 추출\n",
    "b_df.select(\"Id\").show(2) #projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d495255-1180-4a6a-a6e1-a4edb9de7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bdfcca9-cfa3-4f0f-8769-1aedc2c2d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "|     15318|\n",
      "|     21136|\n",
      "|     81156|\n",
      "|     51136|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df.select( expr(\"Hits\")*2 ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e93e38ab-eb11-40c4-8c6b-3971036e4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f07fe",
   "metadata": {},
   "source": [
    "# ➕ 데이터프레임과 임시뷰의 차이\n",
    "\n",
    "## 데이터프레임(DataFrame)\n",
    "데이터프레임은 일종의 표 같은 데이터 구조입니다. 행과 열로 구성되어 있으며, 스프레드시트나 데이터베이스 테이블과 비슷합니다. 데이터프레임을 사용하면 데이터를 쉽게 읽고, 변환하고, 분석할 수 있습니다.\n",
    "\n",
    "### 예시:\n",
    "```python\n",
    "data = [(\"Brook\", 20), (\"Denny\", 31), (\"Jules\", 30)]\n",
    "data_frame = spark.createDataFrame(data)\n",
    "data_frame.show()\n",
    "여기서 data_frame은 이름과 나이 데이터를 포함하는 표처럼 생긴 구조입니다.\n",
    "\n",
    "## 임시뷰(Temporary View)\n",
    "임시뷰는 데이터프레임을 SQL 쿼리에서 사용할 수 있도록 임시로 만든 테이블입니다.\n",
    "SQL을 사용하여 데이터프레임의 데이터를 조회하고 조작할 수 있게 해줍니다.\n",
    "\n",
    "### 예시:\n",
    "python\n",
    "data_frame.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people\")\n",
    "result.show()\n",
    "이 코드에서 createOrReplaceTempView(\"people\")는 data_frame을 \"people\"이라는 이름의 임시 테이블로 만들어 줍니다.  \n",
    "그 후, SQL 쿼리를 사용하여 이 임시 테이블에서 데이터를 조회할 수 있습니다.\n",
    "\n",
    "## 차이점 정리\n",
    "데이터프레임: 데이터를 표 형태로 저장하여 분석하고 조작할 수 있는 구조입니다. 주로 파이썬 코드에서 사용됩니다.\n",
    "\n",
    "임시뷰: 데이터프레임을 SQL 쿼리에서 사용할 수 있도록 임시로 만든 테이블입니다. 주로 SQL을 사용하여 데이터를 조회하고 분석할 때 사용됩니다.\n",
    "\n",
    "## 왜 임시뷰를 사용하는가?\n",
    "간편한 SQL 쿼리 작성:\n",
    "\n",
    "임시뷰를 생성하면 SQL 쿼리를 더 간편하게 작성할 수 있습니다. 데이터프레임을 직접 사용하면 파이썬 코드와 혼합되어 복잡해질 수 있지만, 임시뷰를 사용하면 순수 SQL 쿼리만으로도 데이터를 조회하고 조작할 수 있습니다.\n",
    "\n",
    "복잡한 쿼리 재사용:\n",
    "\n",
    "임시뷰를 사용하면 복잡한 쿼리나 변환을 여러 번 재사용할 수 있습니다. 예를 들어, 여러 SQL 쿼리에서 동일한 데이터를 반복해서 조회해야 하는 경우, 임시뷰를 만들어두면 코드가 더 깔끔하고 가독성이 좋아집니다.\n",
    "\n",
    "SQL 친화적인 인터페이스:\n",
    "\n",
    "SQL에 익숙한 사용자에게는 임시뷰가 더 직관적이고 사용하기 편리할 수 있습니다. 데이터프레임 API보다는 SQL 쿼리를 통해 데이터를 조작하는 것이 더 자연스러운 경우가 많습니다.\n",
    "\n",
    "데이터 분석 도구와의 호환성:\n",
    "\n",
    "임시뷰를 생성하면 다른 데이터 분석 도구와 쉽게 연동할 수 있습니다. 예를 들어, JDBC를 사용하여 SQL 쿼리를 실행하거나, SQL 기반의 데이터 시각화 도구를 사용할 때 유용합니다.\n",
    "\n",
    "## 예시\n",
    "임시뷰를 사용하지 않고 데이터프레임을 직접 SQL 쿼리에서 사용하는 경우, 파이썬 코드와 SQL 코드가 혼합되어 복잡할 수 있습니다:\n",
    "\n",
    "python\n",
    "data_frame.selectExpr(\"name\", \"age\").where(\"age > 25\").show()\n",
    "임시뷰를 사용하면 SQL 쿼리만으로 데이터를 조회할 수 있습니다:\n",
    "\n",
    "python\n",
    "data_frame.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT name, age FROM people WHERE age > 25\")\n",
    "result.show()\n",
    "\n",
    "## 요약\n",
    "데이터프레임: 파이썬 코드 내에서 데이터를 다룰 때 유용합니다.\n",
    "\n",
    "임시뷰: SQL 쿼리를 사용하여 데이터를 다룰 때 유용합니다. 코드 가독성을 높이고, 복잡한 쿼리 재사용이 쉽습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab2f6",
   "metadata": {},
   "source": [
    "# ➕ PySpark에서 STRUCTURE TYPE과 DDL 사용 비교\n",
    "## STRUCTURE TYPE으로 구조 정의\n",
    "STRUCTURE TYPE은 PySpark에서 StructType과 StructField를 사용하여 데이터프레임의 스키마를 정의하는 방법입니다. 이는 데이터의 구조를 명시적으로 정의할 수 있게 해줍니다.\n",
    "\n",
    "### 예시: PySpark의 StructType\n",
    "\n",
    "```\n",
    "python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "# STRUCTURE TYPE 정의\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 데이터프레임 생성\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25)]\n",
    "data_frame = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 데이터프레임 출력\n",
    "data_frame.show()\n",
    "이 예시에서는 StructType과 StructField를 사용하여 데이터프레임의 스키마를 정의합니다. name과 age 필드를 포함한 스키마를 정의하고, 이를 사용하여 데이터프레임을 생성합니다.\n",
    "```\n",
    "## DDL로 구조 정의\n",
    "DDL(Data Definition Language)은 SQL 문법을 사용하여 테이블 등의 구조를 정의하는 것입니다. PySpark에서도 SQL 쿼리를 사용하여 테이블을 생성하고 데이터를 조작할 수 있습니다.\n",
    "\n",
    "### 예시: PySpark SQL DDL\n",
    "```\n",
    "python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "# DDL을 사용하여 테이블 생성\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE Person (\n",
    "    name STRING,\n",
    "    age INT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# 데이터 삽입\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO Person VALUES ('Alice', 30), ('Bob', 25)\n",
    "\"\"\")\n",
    "\n",
    "# 데이터 조회\n",
    "result = spark.sql(\"SELECT * FROM Person\")\n",
    "result.show()\n",
    "이 예시에서는 SQL DDL을 사용하여 Person 테이블을 생성하고 데이터를 삽입한 후, 조회하는 과정을 보여줍니다.\n",
    "```\n",
    "## 차이점 요약\n",
    "### STRUCTURE TYPE:\n",
    "\n",
    "PySpark의 StructType과 StructField를 사용하여 데이터프레임의 스키마를 정의합니다.\n",
    "\n",
    "데이터프레임 생성 시 명시적인 스키마를 정의하여 사용합니다.\n",
    "\n",
    "### DDL:\n",
    "\n",
    "SQL 문법을 사용하여 테이블 등의 구조를 정의합니다.\n",
    "\n",
    "SQL 쿼리를 통해 테이블을 생성, 수정, 삭제합니다.\n",
    "\n",
    "## 왜 STRUCTURE TYPE 또는 DDL을 사용하는가?\n",
    "### STRUCTURE TYPE:\n",
    "\n",
    "데이터프레임을 직접 다룰 때 유용합니다.\n",
    "\n",
    "스키마를 명시적으로 정의하여 데이터의 구조를 명확하게 할 수 있습니다.\n",
    "\n",
    "### DDL:\n",
    "\n",
    "SQL 쿼리를 사용하여 테이블을 정의하고 조작할 때 유용합니다.\n",
    "\n",
    "SQL에 익숙한 사용자에게 편리하며, 테이블 기반의 데이터 조작이 필요할 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd9de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
